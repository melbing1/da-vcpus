diff -rcNP '--exclude=.git' '--exclude=certs' '--exclude=.gitignore' '--exclude=build_kernel_deb.sh' '--exclude=MY_CHANGES' '--exclude=objtool' clean-linux-kernel/linux-5-4-47-unmodified/.gitattributes linux-kernel-v5-4-47/.gitattributes
*** clean-linux-kernel/linux-5-4-47-unmodified/.gitattributes	1969-12-31 19:00:00.000000000 -0500
--- linux-kernel-v5-4-47/.gitattributes	2020-06-19 22:58:59.077194025 -0400
***************
*** 0 ****
--- 1,2 ----
+ *.c   diff=cpp
+ *.h   diff=cpp
diff -rcNP '--exclude=.git' '--exclude=certs' '--exclude=.gitignore' '--exclude=build_kernel_deb.sh' '--exclude=MY_CHANGES' '--exclude=objtool' clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/core.c linux-kernel-v5-4-47/kernel/sched/core.c
*** clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/core.c	2020-06-23 13:54:37.838816463 -0400
--- linux-kernel-v5-4-47/kernel/sched/core.c	2020-06-23 14:38:43.215637641 -0400
***************
*** 2635,2640 ****
--- 2635,2642 ----
  	}
  
  #endif /* CONFIG_SMP */
+ 	
+ 	sp_record_scheduling_event(SP_TRY_TO_WAKE_UP, p->wake_cpu, cpu);
  
  	ttwu_queue(p, cpu, wake_flags);
  unlock:
***************
*** 2940,2945 ****
--- 2942,2948 ----
  {
  	struct rq_flags rf;
  	struct rq *rq;
+ 	int dst_cpu;
  
  	raw_spin_lock_irqsave(&p->pi_lock, rf.flags);
  	p->state = TASK_RUNNING;
***************
*** 2953,2964 ****
  	 * as we're not fully set-up yet.
  	 */
  	p->recent_used_cpu = task_cpu(p);
! 	__set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0));
  #endif
  	rq = __task_rq_lock(p, &rf);
  	update_rq_clock(rq);
  	post_init_entity_util_avg(p);
  
  	activate_task(rq, p, ENQUEUE_NOCLOCK);
  	trace_sched_wakeup_new(p);
  	check_preempt_curr(rq, p, WF_FORK);
--- 2956,2969 ----
  	 * as we're not fully set-up yet.
  	 */
  	p->recent_used_cpu = task_cpu(p);
! 	__set_task_cpu(p, select_task_rq(p, dst_cpu = task_cpu(p), SD_BALANCE_FORK, 0));
! 
  #endif
  	rq = __task_rq_lock(p, &rf);
  	update_rq_clock(rq);
  	post_init_entity_util_avg(p);
  
+ 	sp_record_scheduling_event(SP_WAKE_UP_NEW_TASK, 255, dst_cpu);
  	activate_task(rq, p, ENQUEUE_NOCLOCK);
  	trace_sched_wakeup_new(p);
  	check_preempt_curr(rq, p, WF_FORK);
***************
*** 3502,3507 ****
--- 3507,3515 ----
  		struct migration_arg arg = { p, dest_cpu };
  
  		raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+ 
+         	sp_record_scheduling_event(SP_SCHED_EXEC, task_cpu(p), dest_cpu);
+ 
  		stop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);
  		return;
  	}
diff -rcNP '--exclude=.git' '--exclude=certs' '--exclude=.gitignore' '--exclude=build_kernel_deb.sh' '--exclude=MY_CHANGES' '--exclude=objtool' clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/fair.c linux-kernel-v5-4-47/kernel/sched/fair.c
*** clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/fair.c	2020-06-23 13:54:37.842816453 -0400
--- linux-kernel-v5-4-47/kernel/sched/fair.c	2020-06-23 14:38:43.215637641 -0400
***************
*** 40,45 ****
--- 40,160 ----
  unsigned int sysctl_sched_latency			= 6000000ULL;
  static unsigned int normalized_sysctl_sched_latency	= 6000000ULL;
  
+ #include <linux/module.h>
+ 
+ /******************************************************************************/
+ /* Wrappers                                                                   */
+ /******************************************************************************/
+ struct rq *sp_cpu_rq(int cpu) {
+     return cpu_rq(cpu);
+ }
+ 
+ EXPORT_SYMBOL(sp_cpu_rq);
+ 
+ /******************************************************************************/
+ /* Hook type definitions                                                      */
+ /******************************************************************************/
+ typedef void (*set_nr_running_t)(int *, int, int);
+ typedef void (*record_scheduling_event_t)(int, int, int);
+ typedef void (*record_scheduling_event_extra_t)(int, char, char, char, char,
+                                                      char, char, char, char);
+ typedef void (*record_balancing_event_t)(int, int, uint64_t);
+ typedef void (*record_load_change_t)(unsigned long, int);
+ 
+ /******************************************************************************/
+ /* Hooks                                                                      */
+ /******************************************************************************/
+ __read_mostly volatile set_nr_running_t sp_module_set_nr_running = NULL;
+ __read_mostly volatile record_scheduling_event_t
+               sp_module_record_scheduling_event = NULL;
+ __read_mostly volatile record_scheduling_event_extra_t
+               sp_module_record_scheduling_event_extra = NULL;
+ __read_mostly volatile record_balancing_event_t
+               sp_module_record_balancing_event = NULL;
+ __read_mostly volatile record_load_change_t
+               sp_module_record_load_change = NULL;
+ 
+ /******************************************************************************/
+ /* Default hook implementations                                               */
+ /******************************************************************************/
+ void sp_set_nr_running(int *nr_running_p, int new_nr_running, int dst_cpu)
+ {
+     if (sp_module_set_nr_running)
+         (*sp_module_set_nr_running)(nr_running_p, new_nr_running, dst_cpu);
+     else
+         *nr_running_p = new_nr_running;
+ }
+ 
+ void sp_record_scheduling_event(int event_type, int src_cpu, int dst_cpu)
+ {
+     if (sp_module_record_scheduling_event)
+         (*sp_module_record_scheduling_event)(event_type, src_cpu, dst_cpu);
+ }
+ 
+ void sp_record_scheduling_event_extra(int event_type,
+                 char data1, char data2, char data3, char data4,
+                 char data5, char data6, char data7, char data8)
+ {
+     if (sp_module_record_scheduling_event_extra)
+         (*sp_module_record_scheduling_event_extra)(event_type,
+             data1, data2, data3, data4, data5, data6, data7, data8);
+ }
+ 
+ void sp_record_balancing_event(int event_type, int cpu, uint64_t data)
+ {
+     if (sp_module_record_balancing_event)
+         (*sp_module_record_balancing_event)(event_type, cpu, data);
+ }
+ 
+ void sp_record_load_change(unsigned long load, int cpu)
+ {
+     if (sp_module_record_load_change)
+         (*sp_module_record_load_change)(load, cpu);
+ }
+ 
+ /******************************************************************************/
+ /* Hook setters                                                               */
+ /******************************************************************************/
+ void set_sp_module_set_nr_running(set_nr_running_t __sp_module_set_nr_running)
+ {
+     sp_module_set_nr_running = __sp_module_set_nr_running;
+ 
+ }
+ 
+ void set_sp_module_record_scheduling_event
+     (record_scheduling_event_t __sp_module_record_scheduling_event)
+ {
+     sp_module_record_scheduling_event = __sp_module_record_scheduling_event;
+ }
+ 
+ void set_sp_module_record_scheduling_event_extra
+     (record_scheduling_event_extra_t __sp_module_record_scheduling_event_extra)
+ {
+     sp_module_record_scheduling_event_extra =
+         __sp_module_record_scheduling_event_extra;
+ }
+ 
+ void set_sp_module_record_balancing_event
+     (record_balancing_event_t __sp_module_record_balancing_event)
+ {
+     sp_module_record_balancing_event = __sp_module_record_balancing_event;
+ }
+ 
+ void set_sp_module_record_load_change
+     (record_load_change_t __sp_module_record_load_change)
+ {
+     sp_module_record_load_change = __sp_module_record_load_change;
+ }
+ 
+ /******************************************************************************/
+ /* Symbols                                                                    */
+ /******************************************************************************/
+ EXPORT_SYMBOL(set_sp_module_set_nr_running);
+ EXPORT_SYMBOL(set_sp_module_record_scheduling_event);
+ EXPORT_SYMBOL(set_sp_module_record_scheduling_event_extra);
+ EXPORT_SYMBOL(set_sp_module_record_balancing_event);
+ EXPORT_SYMBOL(set_sp_module_record_load_change);
+ 
  /*
   * The initial- and re-scaling of tunables is configurable
   *
***************
*** 2757,2762 ****
--- 2872,2878 ----
  account_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)
  {
  	update_load_add(&cfs_rq->load, se->load.weight);
+ 	sp_record_load_change(cfs_rq->load.weight, rq_of(cfs_rq)->cpu); //The !parent(se) is no longer used so I think this is the correct place to log. rq_of(cfs_rq)->load was replaced becuase it does not function any more. This function is repeated on line 2891 of account_entity_dequeue()
  #ifdef CONFIG_SMP
  	if (entity_is_task(se)) {
  		struct rq *rq = rq_of(cfs_rq);
***************
*** 2772,2777 ****
--- 2888,2895 ----
  account_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)
  {
  	update_load_sub(&cfs_rq->load, se->load.weight);
+ 	sp_record_load_change(cfs_rq->load.weight, rq_of(cfs_rq)->cpu);
+ 
  #ifdef CONFIG_SMP
  	if (entity_is_task(se)) {
  		account_numa_dequeue(rq_of(cfs_rq), task_of(se));
***************
*** 5589,5594 ****
--- 5707,5713 ----
  	int imbalance_scale = 100 + (sd->imbalance_pct-100)/2;
  	unsigned long imbalance = scale_load_down(NICE_0_LOAD) *
  				(sd->imbalance_pct-100) / 100;
+ 	uint64_t considered_cores = 0;
  
  	do {
  		unsigned long load, avg_load, runnable_load;
***************
*** 5613,5618 ****
--- 5732,5738 ----
  		max_spare_cap = 0;
  
  		for_each_cpu(i, sched_group_span(group)) {
+ 			considered_cores |= (uint64_t)1 << i;
  			load = cpu_runnable_load(cpu_rq(i));
  			runnable_load += load;
  
***************
*** 5660,5665 ****
--- 5780,5787 ----
  		}
  	} while (group = group->next, group != sd->groups);
  
+ 	sp_record_balancing_event(SP_CONSIDERED_CORES_FIG, this_cpu, considered_cores);
+ 
  	/*
  	 * The cross-over point between using spare capacity or least load
  	 * is too conservative for high utilization tasks on partially
***************
*** 5719,5724 ****
--- 5841,5848 ----
  	int least_loaded_cpu = this_cpu;
  	int shallowest_idle_cpu = -1, si_cpu = -1;
  	int i;
+ 	uint64_t considered_cores = 0;
+ 
  
  	/* Check if we have any choice: */
  	if (group->group_weight == 1)
***************
*** 5726,5731 ****
--- 5850,5856 ----
  
  	/* Traverse only the allowed CPUs */
  	for_each_cpu_and(i, sched_group_span(group), p->cpus_ptr) {
+ 		considered_cores |= (uint64_t)1 << i;
  		if (available_idle_cpu(i)) {
  			struct rq *rq = cpu_rq(i);
  			struct cpuidle_state *idle = idle_get_state(rq);
***************
*** 5762,5767 ****
--- 5887,5894 ----
  		}
  	}
  
+ 	sp_record_balancing_event(SP_CONSIDERED_CORES_FIC, this_cpu, considered_cores);
+ 	
  	if (shallowest_idle_cpu != -1)
  		return shallowest_idle_cpu;
  	if (si_cpu != -1)
***************
*** 5818,5824 ****
  				sd = tmp;
  		}
  	}
! 
  	return new_cpu;
  }
  
--- 5945,5951 ----
  				sd = tmp;
  		}
  	}
! 	
  	return new_cpu;
  }
  
***************
*** 6015,6031 ****
  {
  	struct sched_domain *sd;
  	int i, recent_used_cpu;
  
- 	if (available_idle_cpu(target) || sched_idle_cpu(target))
  		return target;
  
  	/*
  	 * If the previous CPU is cache affine and idle, don't be stupid:
  	 */
  	if (prev != target && cpus_share_cache(prev, target) &&
  	    (available_idle_cpu(prev) || sched_idle_cpu(prev)))
  		return prev;
! 
  	/* Check a recently used CPU as a potential idle candidate: */
  	recent_used_cpu = p->recent_used_cpu;
  	if (recent_used_cpu != prev &&
--- 6142,6173 ----
  {
  	struct sched_domain *sd;
  	int i, recent_used_cpu;
+ 	uint64_t considered_cores = 0;
+ 	considered_cores |= (uint64_t)1 << target;
+ 
+ 	if (available_idle_cpu(target) || sched_idle_cpu(target)){
+ 		if (prev > 0)
+ 		{
+ 			sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
+ 		}
  
  		return target;
+ 	}
+ 
+ 	considered_cores |= (uint64_t)1 << i;
  
  	/*
  	 * If the previous CPU is cache affine and idle, don't be stupid:
  	 */
  	if (prev != target && cpus_share_cache(prev, target) &&
  	    (available_idle_cpu(prev) || sched_idle_cpu(prev)))
+ 	{
+ 		if (prev > 0)
+ 		{
+ 			sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
+ 		}
  		return prev;
! 	}
  	/* Check a recently used CPU as a potential idle candidate: */
  	recent_used_cpu = p->recent_used_cpu;
  	if (recent_used_cpu != prev &&
***************
*** 6038,6062 ****
  		 * candidate for the next wake:
  		 */
  		p->recent_used_cpu = prev;
  		return recent_used_cpu;
  	}
  
  	sd = rcu_dereference(per_cpu(sd_llc, target));
  	if (!sd)
  		return target;
! 
  	i = select_idle_core(p, sd, target);
  	if ((unsigned)i < nr_cpumask_bits)
  		return i;
  
  	i = select_idle_cpu(p, sd, target);
  	if ((unsigned)i < nr_cpumask_bits)
  		return i;
! 
  	i = select_idle_smt(p, target);
  	if ((unsigned)i < nr_cpumask_bits)
  		return i;
! 
  	return target;
  }
  
--- 6180,6219 ----
  		 * candidate for the next wake:
  		 */
  		p->recent_used_cpu = prev;
+ 		if (prev > 0) 
+ 		{
+ 			 sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
+ 		}
  		return recent_used_cpu;
  	}
  
  	sd = rcu_dereference(per_cpu(sd_llc, target));
+ 	considered_cores |= (uint64_t)1 << i;
  	if (!sd)
+ 	{
+ 		sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
  		return target;
! 	}
  	i = select_idle_core(p, sd, target);
  	if ((unsigned)i < nr_cpumask_bits)
+ 	{
+ 		sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
  		return i;
+ 	}
  
  	i = select_idle_cpu(p, sd, target);
  	if ((unsigned)i < nr_cpumask_bits)
+ 	{
+ 		sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
  		return i;
! 	}
  	i = select_idle_smt(p, target);
  	if ((unsigned)i < nr_cpumask_bits)
+ 	{
+ 		sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
  		return i;
! 	}
! 	sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
  	return target;
  }
  
***************
*** 6517,6523 ****
  		new_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag);
  	} else if (sd_flag & SD_BALANCE_WAKE) { /* XXX always ? */
  		/* Fast path */
! 
  		new_cpu = select_idle_sibling(p, prev_cpu, new_cpu);
  
  		if (want_affine)
--- 6674,6680 ----
  		new_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag);
  	} else if (sd_flag & SD_BALANCE_WAKE) { /* XXX always ? */
  		/* Fast path */
! 		
  		new_cpu = select_idle_sibling(p, prev_cpu, new_cpu);
  
  		if (want_affine)
***************
*** 7323,7334 ****
  /*
   * detach_task() -- detach the task for the migration specified in env
   */
! static void detach_task(struct task_struct *p, struct lb_env *env)
  {
  	lockdep_assert_held(&env->src_rq->lock);
  
  	deactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);
  	set_task_cpu(p, env->dst_cpu);
  }
  
  /*
--- 7480,7494 ----
  /*
   * detach_task() -- detach the task for the migration specified in env
   */
! static void detach_task(struct task_struct *p, struct lb_env *env, int event_type)
  {
  	lockdep_assert_held(&env->src_rq->lock);
  
  	deactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);
  	set_task_cpu(p, env->dst_cpu);
+ 
+ 	if (event_type >= 0)
+ 		sp_record_scheduling_event(event_type, cpu_of(env->src_rq), env->dst_cpu);
  }
  
  /*
***************
*** 7348,7354 ****
  		if (!can_migrate_task(p, env))
  			continue;
  
! 		detach_task(p, env);
  
  		/*
  		 * Right now, this is only the second place where
--- 7508,7514 ----
  		if (!can_migrate_task(p, env))
  			continue;
  
!         detach_task(p, env, -1);
  
  		/*
  		 * Right now, this is only the second place where
***************
*** 7370,7376 ****
   *
   * Returns number of detached tasks if successful and 0 otherwise.
   */
! static int detach_tasks(struct lb_env *env)
  {
  	struct list_head *tasks = &env->src_rq->cfs_tasks;
  	struct task_struct *p;
--- 7530,7536 ----
   *
   * Returns number of detached tasks if successful and 0 otherwise.
   */
! static int detach_tasks(struct lb_env *env, int event_type)
  {
  	struct list_head *tasks = &env->src_rq->cfs_tasks;
  	struct task_struct *p;
***************
*** 7415,7421 ****
  		if ((load / 2) > env->imbalance)
  			goto next;
  
! 		detach_task(p, env);
  		list_add(&p->se.group_node, &env->tasks);
  
  		detached++;
--- 7575,7581 ----
  		if ((load / 2) > env->imbalance)
  			goto next;
  
!         detach_task(p, env, event_type);
  		list_add(&p->se.group_node, &env->tasks);
  
  		detached++;
***************
*** 8055,8065 ****
--- 8215,8228 ----
  				      int *sg_status)
  {
  	int i, nr_running;
+ 	uint64_t considered_cores;
  
  	memset(sgs, 0, sizeof(*sgs));
  
  	for_each_cpu_and(i, sched_group_span(group), env->cpus) {
  		struct rq *rq = cpu_rq(i);
+ 		
+ 		considered_cores |= (uint64_t)1 << i;
  
  		if ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))
  			env->flags |= LBF_NOHZ_AGAIN;
***************
*** 8092,8097 ****
--- 8255,8263 ----
  		}
  	}
  
+     sp_record_balancing_event(SP_CONSIDERED_CORES_USLS, env->dst_cpu,
+                                considered_cores);
+ 
  	/* Adjust by relative CPU capacity of the group */
  	sgs->group_capacity = group->sgc->capacity;
  	sgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;
***************
*** 8639,8649 ****
--- 8805,8818 ----
  	struct rq *busiest = NULL, *rq;
  	unsigned long busiest_load = 0, busiest_capacity = 1;
  	int i;
+ 	uint64_t considered_cores = 0;
  
  	for_each_cpu_and(i, sched_group_span(group), env->cpus) {
  		unsigned long capacity, load;
  		enum fbq_type rt;
  
+ 		considered_cores |= (uint64_t)1 << i;
+ 
  		rq = cpu_rq(i);
  		rt = fbq_classify_rq(rq);
  
***************
*** 8724,8729 ****
--- 8893,8901 ----
  		}
  	}
  
+     sp_record_balancing_event(SP_CONSIDERED_CORES_FBQ, env->dst_cpu,
+                               considered_cores);
+ 
  	return busiest;
  }
  
***************
*** 8828,8834 ****
   */
  static int load_balance(int this_cpu, struct rq *this_rq,
  			struct sched_domain *sd, enum cpu_idle_type idle,
! 			int *continue_balancing)
  {
  	int ld_moved, cur_ld_moved, active_balance = 0;
  	struct sched_domain *sd_parent = sd->parent;
--- 9000,9006 ----
   */
  static int load_balance(int this_cpu, struct rq *this_rq,
  			struct sched_domain *sd, enum cpu_idle_type idle,
!             int *continue_balancing, int event_type)
  {
  	int ld_moved, cur_ld_moved, active_balance = 0;
  	struct sched_domain *sd_parent = sd->parent;
***************
*** 8897,8903 ****
  		 * cur_ld_moved - load moved in current iteration
  		 * ld_moved     - cumulative load moved across iterations
  		 */
! 		cur_ld_moved = detach_tasks(&env);
  
  		/*
  		 * We've detached some tasks from busiest_rq. Every
--- 9069,9075 ----
  		 * cur_ld_moved - load moved in current iteration
  		 * ld_moved     - cumulative load moved across iterations
  		 */
!         cur_ld_moved = detach_tasks(&env, event_type + SP_MOVE_TASKS);
  
  		/*
  		 * We've detached some tasks from busiest_rq. Every
***************
*** 9029,9034 ****
--- 9201,9207 ----
  			raw_spin_unlock_irqrestore(&busiest->lock, flags);
  
  			if (active_balance) {
+                 		sp_record_scheduling_event(event_type + SP_ACTIVE_LOAD_BALANCE_CPU_STOP, cpu_of(busiest), this_cpu);
  				stop_one_cpu_nowait(cpu_of(busiest),
  					active_load_balance_cpu_stop, busiest,
  					&busiest->active_balance_work);
***************
*** 9285,9291 ****
  		}
  
  		if (time_after_eq(jiffies, sd->last_balance + interval)) {
! 			if (load_balance(cpu, rq, sd, idle, &continue_balancing)) {
  				/*
  				 * The LBF_DST_PINNED logic could have changed
  				 * env->dst_cpu, so we can't know our idle
--- 9458,9464 ----
  		}
  
  		if (time_after_eq(jiffies, sd->last_balance + interval)) {
! 			if (load_balance(cpu, rq, sd, idle, &continue_balancing, SP_REBALANCE_DOMAINS)) { 
  				/*
  				 * The LBF_DST_PINNED logic could have changed
  				 * env->dst_cpu, so we can't know our idle
***************
*** 9860,9866 ****
  
  			pulled_task = load_balance(this_cpu, this_rq,
  						   sd, CPU_NEWLY_IDLE,
! 						   &continue_balancing);
  
  			domain_cost = sched_clock_cpu(this_cpu) - t0;
  			if (domain_cost > sd->max_newidle_lb_cost)
--- 10033,10039 ----
  
  			pulled_task = load_balance(this_cpu, this_rq,
  						   sd, CPU_NEWLY_IDLE,
!                            &continue_balancing, SP_IDLE_BALANCE);
  
  			domain_cost = sched_clock_cpu(this_cpu) - t0;
  			if (domain_cost > sd->max_newidle_lb_cost)
diff -rcNP '--exclude=.git' '--exclude=certs' '--exclude=.gitignore' '--exclude=build_kernel_deb.sh' '--exclude=MY_CHANGES' '--exclude=objtool' clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/sched.h linux-kernel-v5-4-47/kernel/sched/sched.h
*** clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/sched.h	2020-06-23 13:54:37.842816453 -0400
--- linux-kernel-v5-4-47/kernel/sched/sched.h	2020-07-24 16:41:13.159653393 -0400
***************
*** 81,86 ****
--- 81,109 ----
  # define SCHED_WARN_ON(x)	({ (void)(x), 0; })
  #endif
  
+ extern void sp_set_nr_running(int *nr_running_p, int nr_running, int dst_cpu);
+ extern void sp_record_scheduling_event(int event_type, int src_cpu,
+                                         int dst_cpu);
+ extern void sp_record_scheduling_event_extra(int event_type,
+                 char data1, char data2, char data3, char data4,
+                 char data5, char data6, char data7, char data8);
+ extern void sp_record_load_change(unsigned long load, int cpu);
+ 
+ enum {
+     SP_SCHED_EXEC = 0,
+     SP_TRY_TO_WAKE_UP,
+     SP_WAKE_UP_NEW_TASK,
+     SP_IDLE_BALANCE,
+     SP_REBALANCE_DOMAINS,
+     SP_MOVE_TASKS = 10,
+     SP_ACTIVE_LOAD_BALANCE_CPU_STOP = 20,
+     SP_CONSIDERED_CORES_SIS = 200,
+     SP_CONSIDERED_CORES_USLS,
+     SP_CONSIDERED_CORES_FBQ,
+     SP_CONSIDERED_CORES_FIG,
+     SP_CONSIDERED_CORES_FIC
+ };
+ 
  struct rq;
  struct cpuidle_state;
  
***************
*** 1929,1935 ****
  {
  	unsigned prev_nr = rq->nr_running;
  
! 	rq->nr_running = prev_nr + count;
  
  #ifdef CONFIG_SMP
  	if (prev_nr < 2 && rq->nr_running >= 2) {
--- 1952,1958 ----
  {
  	unsigned prev_nr = rq->nr_running;
  
!     	sp_set_nr_running(&rq->nr_running, prev_nr + count, cpu_of(rq));
  
  #ifdef CONFIG_SMP
  	if (prev_nr < 2 && rq->nr_running >= 2) {
***************
*** 1943,1949 ****
  
  static inline void sub_nr_running(struct rq *rq, unsigned count)
  {
! 	rq->nr_running -= count;
  	/* Check if we still need preemption */
  	sched_update_tick_dependency(rq);
  }
--- 1966,1975 ----
  
  static inline void sub_nr_running(struct rq *rq, unsigned count)
  {
! 	//rq->nr_running -= count;
! 	
! 	sp_set_nr_running(&rq->nr_running, rq->nr_running - count, cpu_of(rq));
! 
  	/* Check if we still need preemption */
  	sched_update_tick_dependency(rq);
  }
