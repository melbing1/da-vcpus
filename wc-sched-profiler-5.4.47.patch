diff -uprNw '--exclude=certs' '--exclude=objtool' linux-kernel-v5-4-47/.gitattributes clean-linux-kernel/linux-5-4-47-unmodified/.gitattributes
--- linux-kernel-v5-4-47/.gitattributes	2020-06-19 22:58:59.000000000 -0400
+++ clean-linux-kernel/linux-5-4-47-unmodified/.gitattributes	1969-12-31 19:00:00.000000000 -0500
@@ -1,2 +0,0 @@
-*.c   diff=cpp
-*.h   diff=cpp
diff -uprNw '--exclude=certs' '--exclude=objtool' linux-kernel-v5-4-47/kernel/sched/core.c clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/core.c
--- linux-kernel-v5-4-47/kernel/sched/core.c	2020-06-23 14:38:43.000000000 -0400
+++ clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/core.c	2020-06-23 13:54:37.000000000 -0400
@@ -2636,8 +2636,6 @@ try_to_wake_up(struct task_struct *p, un
 
 #endif /* CONFIG_SMP */
 	
-	sp_record_scheduling_event(SP_TRY_TO_WAKE_UP, p->wake_cpu, cpu);
-
 	ttwu_queue(p, cpu, wake_flags);
 unlock:
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
@@ -2942,7 +2940,6 @@ void wake_up_new_task(struct task_struct
 {
 	struct rq_flags rf;
 	struct rq *rq;
-	int dst_cpu;
 
 	raw_spin_lock_irqsave(&p->pi_lock, rf.flags);
 	p->state = TASK_RUNNING;
@@ -2956,14 +2953,12 @@ void wake_up_new_task(struct task_struct
 	 * as we're not fully set-up yet.
 	 */
 	p->recent_used_cpu = task_cpu(p);
-	__set_task_cpu(p, select_task_rq(p, dst_cpu = task_cpu(p), SD_BALANCE_FORK, 0));
-
+	__set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0));
 #endif
 	rq = __task_rq_lock(p, &rf);
 	update_rq_clock(rq);
 	post_init_entity_util_avg(p);
 
-	sp_record_scheduling_event(SP_WAKE_UP_NEW_TASK, 255, dst_cpu);
 	activate_task(rq, p, ENQUEUE_NOCLOCK);
 	trace_sched_wakeup_new(p);
 	check_preempt_curr(rq, p, WF_FORK);
@@ -3507,9 +3502,6 @@ void sched_exec(void)
 		struct migration_arg arg = { p, dest_cpu };
 
 		raw_spin_unlock_irqrestore(&p->pi_lock, flags);
-
-        	sp_record_scheduling_event(SP_SCHED_EXEC, task_cpu(p), dest_cpu);
-
 		stop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);
 		return;
 	}
diff -uprNw '--exclude=certs' '--exclude=objtool' linux-kernel-v5-4-47/kernel/sched/fair.c clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/fair.c
--- linux-kernel-v5-4-47/kernel/sched/fair.c	2020-08-13 17:43:48.184672003 -0400
+++ clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/fair.c	2020-06-23 13:54:37.000000000 -0400
@@ -40,121 +40,6 @@
 unsigned int sysctl_sched_latency			= 6000000ULL;
 static unsigned int normalized_sysctl_sched_latency	= 6000000ULL;
 
-#include <linux/module.h>
-
-/******************************************************************************/
-/* Wrappers                                                                   */
-/******************************************************************************/
-struct rq *sp_cpu_rq(int cpu) {
-    return cpu_rq(cpu);
-}
-
-EXPORT_SYMBOL(sp_cpu_rq);
-
-/******************************************************************************/
-/* Hook type definitions                                                      */
-/******************************************************************************/
-typedef void (*set_nr_running_t)(int *, int, int);
-typedef void (*record_scheduling_event_t)(int, int, int);
-typedef void (*record_scheduling_event_extra_t)(int, char, char, char, char,
-                                                     char, char, char, char);
-typedef void (*record_balancing_event_t)(int, int, uint64_t);
-typedef void (*record_load_change_t)(unsigned long, int);
-
-/******************************************************************************/
-/* Hooks                                                                      */
-/******************************************************************************/
-__read_mostly volatile set_nr_running_t sp_module_set_nr_running = NULL;
-__read_mostly volatile record_scheduling_event_t
-              sp_module_record_scheduling_event = NULL;
-__read_mostly volatile record_scheduling_event_extra_t
-              sp_module_record_scheduling_event_extra = NULL;
-__read_mostly volatile record_balancing_event_t
-              sp_module_record_balancing_event = NULL;
-__read_mostly volatile record_load_change_t
-              sp_module_record_load_change = NULL;
-
-/******************************************************************************/
-/* Default hook implementations                                               */
-/******************************************************************************/
-void sp_set_nr_running(int *nr_running_p, int new_nr_running, int dst_cpu)
-{
-    if (sp_module_set_nr_running)
-        (*sp_module_set_nr_running)(nr_running_p, new_nr_running, dst_cpu);
-    else
-        *nr_running_p = new_nr_running;
-}
-
-void sp_record_scheduling_event(int event_type, int src_cpu, int dst_cpu)
-{
-    if (sp_module_record_scheduling_event)
-        (*sp_module_record_scheduling_event)(event_type, src_cpu, dst_cpu);
-}
-
-void sp_record_scheduling_event_extra(int event_type,
-                char data1, char data2, char data3, char data4,
-                char data5, char data6, char data7, char data8)
-{
-    if (sp_module_record_scheduling_event_extra)
-        (*sp_module_record_scheduling_event_extra)(event_type,
-            data1, data2, data3, data4, data5, data6, data7, data8);
-}
-
-void sp_record_balancing_event(int event_type, int cpu, uint64_t data)
-{
-    if (sp_module_record_balancing_event)
-        (*sp_module_record_balancing_event)(event_type, cpu, data);
-}
-
-void sp_record_load_change(unsigned long load, int cpu)
-{
-    if (sp_module_record_load_change)
-        (*sp_module_record_load_change)(load, cpu);
-}
-
-/******************************************************************************/
-/* Hook setters                                                               */
-/******************************************************************************/
-void set_sp_module_set_nr_running(set_nr_running_t __sp_module_set_nr_running)
-{
-    sp_module_set_nr_running = __sp_module_set_nr_running;
-
-}
-
-void set_sp_module_record_scheduling_event
-    (record_scheduling_event_t __sp_module_record_scheduling_event)
-{
-    sp_module_record_scheduling_event = __sp_module_record_scheduling_event;
-}
-
-void set_sp_module_record_scheduling_event_extra
-    (record_scheduling_event_extra_t __sp_module_record_scheduling_event_extra)
-{
-    sp_module_record_scheduling_event_extra =
-        __sp_module_record_scheduling_event_extra;
-}
-
-void set_sp_module_record_balancing_event
-    (record_balancing_event_t __sp_module_record_balancing_event)
-{
-    sp_module_record_balancing_event = __sp_module_record_balancing_event;
-}
-
-void set_sp_module_record_load_change
-    (record_load_change_t __sp_module_record_load_change)
-{
-    sp_module_record_load_change = __sp_module_record_load_change;
-}
-
-/******************************************************************************/
-/* Symbols                                                                    */
-/******************************************************************************/
-EXPORT_SYMBOL(set_sp_module_set_nr_running);
-EXPORT_SYMBOL(set_sp_module_record_scheduling_event);
-EXPORT_SYMBOL(set_sp_module_record_scheduling_event_extra);
-EXPORT_SYMBOL(set_sp_module_record_balancing_event);
-EXPORT_SYMBOL(set_sp_module_record_load_change);
-
 /*
  * The initial- and re-scaling of tunables is configurable
  *
@@ -233,25 +118,22 @@ int __weak arch_asym_cpu_priority(int cp
 unsigned int sysctl_sched_cfs_bandwidth_slice		= 5000UL;
 #endif
 
-static inline void update_load_add(struct load_weight *lw, unsigned long inc, int cpu)
+static inline void update_load_add(struct load_weight *lw, unsigned long inc)
 {	
 	lw->weight += inc;
 	lw->inv_weight = 0;
-	sp_record_load_change(lw->weight, cpu);
 }
 
-static inline void update_load_sub(struct load_weight *lw, unsigned long dec, int cpu)
+static inline void update_load_sub(struct load_weight *lw, unsigned long dec)
 {
 	lw->weight -= dec;
 	lw->inv_weight = 0;
-	sp_record_load_change(lw->weight, cpu);
 }
 
-static inline void update_load_set(struct load_weight *lw, unsigned long w, int cpu)
+static inline void update_load_set(struct load_weight *lw, unsigned long w)
 {
 	lw->weight = w;
 	lw->inv_weight = 0;
-	sp_record_load_change(lw->weight, cpu);
 }
 
 /*
@@ -821,9 +703,8 @@ static u64 sched_slice(struct cfs_rq *cf
 		if (unlikely(!se->on_rq)) {
 			lw = cfs_rq->load;
 
-			update_load_add(&lw, se->load.weight, rq_of(cfs_rq)->cpu);
+			update_load_add(&lw, se->load.weight);
 			load = &lw;
-			sp_record_load_change(se->load.weight, rq_of(cfs_rq)->cpu); //Unclear if this is needed, but since it simply sets a value so I do not think that is a problem
 		}
 		slice = __calc_delta(slice, se->load.weight, load);
 	}
@@ -2875,8 +2756,7 @@ static inline void update_scan_period(st
 static void
 account_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)
 {
-	update_load_add(&cfs_rq->load, se->load.weight, rq_of(cfs_rq)->cpu);
-	sp_record_load_change(cfs_rq->load.weight, rq_of(cfs_rq)->cpu); 
+	update_load_add(&cfs_rq->load, se->load.weight);
 #ifdef CONFIG_SMP
 	if (entity_is_task(se)) {
 		struct rq *rq = rq_of(cfs_rq);
@@ -2891,10 +2771,7 @@ account_entity_enqueue(struct cfs_rq *cf
 static void
 account_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)
 {
-	update_load_sub(&cfs_rq->load, se->load.weight, rq_of(cfs_rq)->cpu);
-	sp_record_load_change(cfs_rq->load.weight, rq_of(cfs_rq)->cpu);
-
-
+	update_load_sub(&cfs_rq->load, se->load.weight);
 #ifdef CONFIG_SMP
 	if (entity_is_task(se)) {
 		account_numa_dequeue(rq_of(cfs_rq), task_of(se));
@@ -2902,7 +2779,6 @@ account_entity_dequeue(struct cfs_rq *cf
 	}
 #endif
 	cfs_rq->nr_running--;
-	sp_record_load_change(cfs_rq->load.weight, rq_of(cfs_rq)->cpu);
 }
 
 /*
@@ -3010,7 +2886,7 @@ static void reweight_entity(struct cfs_r
 	dequeue_load_avg(cfs_rq, se);
 
 	se->runnable_weight = runnable;
-	update_load_set(&se->load, weight, rq_of(cfs_rq)->cpu);
+	update_load_set(&se->load, weight);
 
 #ifdef CONFIG_SMP
 	do {
@@ -5713,7 +5589,6 @@ find_idlest_group(struct sched_domain *s
 	int imbalance_scale = 100 + (sd->imbalance_pct-100)/2;
 	unsigned long imbalance = scale_load_down(NICE_0_LOAD) *
 				(sd->imbalance_pct-100) / 100;
-	uint64_t considered_cores = 0;
 
 	do {
 		unsigned long load, avg_load, runnable_load;
@@ -5738,7 +5613,6 @@ find_idlest_group(struct sched_domain *s
 		max_spare_cap = 0;
 
 		for_each_cpu(i, sched_group_span(group)) {
-			considered_cores |= (uint64_t)1 << i;
 			load = cpu_runnable_load(cpu_rq(i));
 			runnable_load += load;
 
@@ -5786,8 +5660,6 @@ find_idlest_group(struct sched_domain *s
 		}
 	} while (group = group->next, group != sd->groups);
 
-	sp_record_balancing_event(SP_CONSIDERED_CORES_FIG, this_cpu, considered_cores);
-
 	/*
 	 * The cross-over point between using spare capacity or least load
 	 * is too conservative for high utilization tasks on partially
@@ -5847,8 +5719,6 @@ find_idlest_group_cpu(struct sched_group
 	int least_loaded_cpu = this_cpu;
 	int shallowest_idle_cpu = -1, si_cpu = -1;
 	int i;
-	uint64_t considered_cores = 0;
-
 
 	/* Check if we have any choice: */
 	if (group->group_weight == 1)
@@ -5856,7 +5726,6 @@ find_idlest_group_cpu(struct sched_group
 
 	/* Traverse only the allowed CPUs */
 	for_each_cpu_and(i, sched_group_span(group), p->cpus_ptr) {
-		considered_cores |= (uint64_t)1 << i;
 		if (available_idle_cpu(i)) {
 			struct rq *rq = cpu_rq(i);
 			struct cpuidle_state *idle = idle_get_state(rq);
@@ -5893,8 +5762,6 @@ find_idlest_group_cpu(struct sched_group
 		}
 	}
 
-	sp_record_balancing_event(SP_CONSIDERED_CORES_FIC, this_cpu, considered_cores);
-	
 	if (shallowest_idle_cpu != -1)
 		return shallowest_idle_cpu;
 	if (si_cpu != -1)
@@ -6148,32 +6015,17 @@ static int select_idle_sibling(struct ta
 {
 	struct sched_domain *sd;
 	int i, recent_used_cpu;
-	uint64_t considered_cores = 0;
-	considered_cores |= (uint64_t)1 << target;
-
-	if (available_idle_cpu(target) || sched_idle_cpu(target)){
-		if (prev > 0)
-		{
-			sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
-		}
 
+	if (available_idle_cpu(target) || sched_idle_cpu(target))
 		return target;
-	}
-
-	considered_cores |= (uint64_t)1 << i;
 
 	/*
 	 * If the previous CPU is cache affine and idle, don't be stupid:
 	 */
 	if (prev != target && cpus_share_cache(prev, target) &&
 	    (available_idle_cpu(prev) || sched_idle_cpu(prev)))
-	{
-		if (prev > 0)
-		{
-			sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
-		}
 		return prev;
-	}
+
 	/* Check a recently used CPU as a potential idle candidate: */
 	recent_used_cpu = p->recent_used_cpu;
 	if (recent_used_cpu != prev &&
@@ -6186,40 +6038,25 @@ static int select_idle_sibling(struct ta
 		 * candidate for the next wake:
 		 */
 		p->recent_used_cpu = prev;
-		if (prev > 0) 
-		{
-			 sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
-		}
 		return recent_used_cpu;
 	}
 
 	sd = rcu_dereference(per_cpu(sd_llc, target));
-	considered_cores |= (uint64_t)1 << i;
 	if (!sd)
-	{
-		sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
 		return target;
-	}
+
 	i = select_idle_core(p, sd, target);
 	if ((unsigned)i < nr_cpumask_bits)
-	{
-		sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
 		return i;
-	}
 
 	i = select_idle_cpu(p, sd, target);
 	if ((unsigned)i < nr_cpumask_bits)
-	{
-		sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
 		return i;
-	}
+
 	i = select_idle_smt(p, target);
 	if ((unsigned)i < nr_cpumask_bits)
-	{
-		sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
 		return i;
-	}
-	sp_record_balancing_event(SP_CONSIDERED_CORES_SIS, prev, considered_cores);
+
 	return target;
 }
 
@@ -7486,15 +7323,12 @@ int can_migrate_task(struct task_struct
 /*
  * detach_task() -- detach the task for the migration specified in env
  */
-static void detach_task(struct task_struct *p, struct lb_env *env, int event_type)
+static void detach_task(struct task_struct *p, struct lb_env *env)
 {
 	lockdep_assert_held(&env->src_rq->lock);
 
 	deactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);
 	set_task_cpu(p, env->dst_cpu);
-
-	if (event_type >= 0)
-		sp_record_scheduling_event(event_type, cpu_of(env->src_rq), env->dst_cpu);
 }
 
 /*
@@ -7514,7 +7348,7 @@ static struct task_struct *detach_one_ta
 		if (!can_migrate_task(p, env))
 			continue;
 
-        detach_task(p, env, -1);
+		detach_task(p, env);
 
 		/*
 		 * Right now, this is only the second place where
@@ -7536,7 +7370,7 @@ static const unsigned int sched_nr_migra
  *
  * Returns number of detached tasks if successful and 0 otherwise.
  */
-static int detach_tasks(struct lb_env *env, int event_type)
+static int detach_tasks(struct lb_env *env)
 {
 	struct list_head *tasks = &env->src_rq->cfs_tasks;
 	struct task_struct *p;
@@ -7581,7 +7415,7 @@ static int detach_tasks(struct lb_env *e
 		if ((load / 2) > env->imbalance)
 			goto next;
 
-        detach_task(p, env, event_type);
+		detach_task(p, env);
 		list_add(&p->se.group_node, &env->tasks);
 
 		detached++;
@@ -8221,15 +8055,12 @@ static inline void update_sg_lb_stats(st
 				      int *sg_status)
 {
 	int i, nr_running;
-	uint64_t considered_cores;
 
 	memset(sgs, 0, sizeof(*sgs));
 
 	for_each_cpu_and(i, sched_group_span(group), env->cpus) {
 		struct rq *rq = cpu_rq(i);
 		
-		considered_cores |= (uint64_t)1 << i;
-
 		if ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))
 			env->flags |= LBF_NOHZ_AGAIN;
 
@@ -8261,9 +8092,6 @@ static inline void update_sg_lb_stats(st
 		}
 	}
 
-    sp_record_balancing_event(SP_CONSIDERED_CORES_USLS, env->dst_cpu,
-                               considered_cores);
-
 	/* Adjust by relative CPU capacity of the group */
 	sgs->group_capacity = group->sgc->capacity;
 	sgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;
@@ -8811,14 +8639,11 @@ static struct rq *find_busiest_queue(str
 	struct rq *busiest = NULL, *rq;
 	unsigned long busiest_load = 0, busiest_capacity = 1;
 	int i;
-	uint64_t considered_cores = 0;
 
 	for_each_cpu_and(i, sched_group_span(group), env->cpus) {
 		unsigned long capacity, load;
 		enum fbq_type rt;
 
-		considered_cores |= (uint64_t)1 << i;
-
 		rq = cpu_rq(i);
 		rt = fbq_classify_rq(rq);
 
@@ -8899,9 +8724,6 @@ static struct rq *find_busiest_queue(str
 		}
 	}
 
-    sp_record_balancing_event(SP_CONSIDERED_CORES_FBQ, env->dst_cpu,
-                              considered_cores);
-
 	return busiest;
 }
 
@@ -9006,7 +8828,7 @@ static int should_we_balance(struct lb_e
  */
 static int load_balance(int this_cpu, struct rq *this_rq,
 			struct sched_domain *sd, enum cpu_idle_type idle,
-            int *continue_balancing, int event_type)
+			int *continue_balancing)
 {
 	int ld_moved, cur_ld_moved, active_balance = 0;
 	struct sched_domain *sd_parent = sd->parent;
@@ -9075,7 +8897,7 @@ more_balance:
 		 * cur_ld_moved - load moved in current iteration
 		 * ld_moved     - cumulative load moved across iterations
 		 */
-        cur_ld_moved = detach_tasks(&env, event_type + SP_MOVE_TASKS);
+		cur_ld_moved = detach_tasks(&env);
 
 		/*
 		 * We've detached some tasks from busiest_rq. Every
@@ -9207,7 +9029,6 @@ more_balance:
 			raw_spin_unlock_irqrestore(&busiest->lock, flags);
 
 			if (active_balance) {
-                		sp_record_scheduling_event(event_type + SP_ACTIVE_LOAD_BALANCE_CPU_STOP, cpu_of(busiest), this_cpu);
 				stop_one_cpu_nowait(cpu_of(busiest),
 					active_load_balance_cpu_stop, busiest,
 					&busiest->active_balance_work);
@@ -9464,7 +9285,7 @@ static void rebalance_domains(struct rq
 		}
 
 		if (time_after_eq(jiffies, sd->last_balance + interval)) {
-			if (load_balance(cpu, rq, sd, idle, &continue_balancing, SP_REBALANCE_DOMAINS)) { 
+			if (load_balance(cpu, rq, sd, idle, &continue_balancing)) {
 				/*
 				 * The LBF_DST_PINNED logic could have changed
 				 * env->dst_cpu, so we can't know our idle
@@ -10039,7 +9860,7 @@ int newidle_balance(struct rq *this_rq,
 
 			pulled_task = load_balance(this_cpu, this_rq,
 						   sd, CPU_NEWLY_IDLE,
-                           &continue_balancing, SP_IDLE_BALANCE);
+						   &continue_balancing);
 
 			domain_cost = sched_clock_cpu(this_cpu) - t0;
 			if (domain_cost > sd->max_newidle_lb_cost)
@@ -10565,7 +10386,7 @@ void init_tg_cfs_entry(struct task_group
 
 	se->my_q = cfs_rq;
 	/* guarantee group entities always have weight */
-	update_load_set(&se->load, NICE_0_LOAD, rq_of(cfs_rq)->cpu);
+	update_load_set(&se->load, NICE_0_LOAD);
 	se->parent = parent;
 }
 
diff -uprNw '--exclude=certs' '--exclude=objtool' linux-kernel-v5-4-47/kernel/sched/sched.h clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/sched.h
--- linux-kernel-v5-4-47/kernel/sched/sched.h	2020-07-24 16:41:13.000000000 -0400
+++ clean-linux-kernel/linux-5-4-47-unmodified/kernel/sched/sched.h	2020-06-23 13:54:37.000000000 -0400
@@ -81,29 +81,6 @@
 # define SCHED_WARN_ON(x)	({ (void)(x), 0; })
 #endif
 
-extern void sp_set_nr_running(int *nr_running_p, int nr_running, int dst_cpu);
-extern void sp_record_scheduling_event(int event_type, int src_cpu,
-                                        int dst_cpu);
-extern void sp_record_scheduling_event_extra(int event_type,
-                char data1, char data2, char data3, char data4,
-                char data5, char data6, char data7, char data8);
-extern void sp_record_load_change(unsigned long load, int cpu);
-
-enum {
-    SP_SCHED_EXEC = 0,
-    SP_TRY_TO_WAKE_UP,
-    SP_WAKE_UP_NEW_TASK,
-    SP_IDLE_BALANCE,
-    SP_REBALANCE_DOMAINS,
-    SP_MOVE_TASKS = 10,
-    SP_ACTIVE_LOAD_BALANCE_CPU_STOP = 20,
-    SP_CONSIDERED_CORES_SIS = 200,
-    SP_CONSIDERED_CORES_USLS,
-    SP_CONSIDERED_CORES_FBQ,
-    SP_CONSIDERED_CORES_FIG,
-    SP_CONSIDERED_CORES_FIC
-};
-
 struct rq;
 struct cpuidle_state;
 
@@ -1952,7 +1929,7 @@ static inline void add_nr_running(struct
 {
 	unsigned prev_nr = rq->nr_running;
 
-    	sp_set_nr_running(&rq->nr_running, prev_nr + count, cpu_of(rq));
+	rq->nr_running = prev_nr + count;
 
 #ifdef CONFIG_SMP
 	if (prev_nr < 2 && rq->nr_running >= 2) {
@@ -1966,10 +1943,7 @@ static inline void add_nr_running(struct
 
 static inline void sub_nr_running(struct rq *rq, unsigned count)
 {
-	//rq->nr_running -= count;
-	
-	sp_set_nr_running(&rq->nr_running, rq->nr_running - count, cpu_of(rq));
-
+	rq->nr_running -= count;
 	/* Check if we still need preemption */
 	sched_update_tick_dependency(rq);
 }
